---
title: "Final Project"
author: "Colden Johnson"
date: "2022-12-11"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/colde/OneDrive/Documents/DKU Labs")
library(tidyverse)
library(dplyr)
library(corrplot)
library(memisc)
polypar <- read_csv("finalproject.csv")

```

## Part A - Short Answer Questions
## Question 1
```{r}
grouped_polypar <- polypar %>%
  group_by(district) %>%
  summarize(mean_participation = mean(participates))
grouped_polypar <- grouped_polypar[order(grouped_polypar$mean_participation, decreasing = TRUE),]
grouped_polypar[1:10,]
```
## Question 2

```{r Political Variation by city}

ggplot(polypar, aes(city, participates)) +
  geom_boxplot(fill = c('light blue', 'light green')) +
  ggtitle('Political Participation by City')
```
```{r, fig.height = 2, fig.width = 4}
greenville <- polypar %>%
  filter(city == 'Greenville')
blueville <- polypar %>%
  filter(city == 'Blueville')

# before conducting a t-test, we need to a) check for normality and b) check for normal variance (we can use an f-test to check this)
ggplot(greenville, aes(x=participates)) +
  geom_histogram(bins = 20, fill = 'steelblue') +
  xlab('Participation') +
  ylab('Frequency') +
  ggtitle('Greenville')
ggplot(blueville, aes(x=participates)) +
  geom_histogram(bins = 20, fill = 'steelblue') +
  xlab('Participation') +
  ylab('Frequency') +
  ggtitle('Blueville')
var.test(blueville$participates, greenville$participates)

t.test(greenville$participates, blueville$participates, method = 'two-side')
```

Because the values of participates are bounded by 0 and 10, the tails of both boxplots end at these values. It appears that Greenville's mean and middle 2 quartiles (middle 50%) are both shifted somewhat higher than blueville's, and that in general greenville has a higher participates score. We can run a t-test to determine if this is a statistically significant difference. 

Given the low p-value of the t-test, the null hypothesis is rejected. The 95% confidence interval falls between 0.24 and 0.61. This means we are 95% confident the true difference in means between blueville and greenville participation falls between 0.24 and 0.61.

## Question 3

## Variable Types
Age - quantitative discrete --- discrete values of whole age numbers. However, number values that vary between 18-99.
City - nominal categorical --- There is no intrinsic order, but two or more distinct categories.
City_home - ordinal categorical --- because this is a ranked list, there is an intrinsic order (ordinal), and this order is separated into categories.
District - nominal categorical (note: this variable could be treated as an identifier, if we grouped by district into categories) -- There is no intrinsic order, but data is separated into a number of distinct categories.
Educ - nominal categorical -- There are two distinct categories, but no order for the categories to be ranked in. This is a boolean value.

## Question 4

```{r}
m1 <- lm(political_efficacy ~ political_interest, data = polypar)
m2 <- lm(political_efficacy ~ trust_city_gov * political_interest, data = polypar)
summary(m1)
summary(m2)

```

In the first model, the intercept of political_efficacy is 2.84. Every unit increase in political_interest is associated with a decrease of 0.06 in political_efficacy. This is semi-counterintuitive, but this doesn't mean much because a) the affect size is quite small, and b) the p-value is not significant.

In the second model, the intercept or predicted value when all 3 other values are 0 is predicted to be 2.23. As trust_city_gov increases by 1 unit, political efficacy is predicted to increase by 0.27. Every unit increase in political_interest is associated with a 0.17 increase in political_efficacy. The interaction between trust_city_gov:political_interest can be interpreted by as trust_city_gov increases by 1 unit the slope between trust_city_gov and political_interest decreases by 0.10. Political interest and trust_city_gov both appear to positively predict political efficacy, but when taken together have a lesser impact (according to this model). We can't say with confidence whether Eqvotey is right, given that the R squared value for both regressions is abysmally low (~1% variance explained).

## Question 5

```{r T Test}
set.seed(54321)
pconfint = function(phat,n,conf=0.95) {
  se = sqrt(phat*(1-phat)/n)
  al2 = 1-(1-conf)/2
  zstar = qnorm(al2)
  upperlimit = phat+zstar*se
  lowerlimit = phat - zstar*se
  return(c(round(lowerlimit,3), round(upperlimit, 3)))
}
phatPrpnReturn = mean(polypar$pol_active_village, na.rm = T)
pconfint(phat = phatPrpnReturn, n = 1969, conf = 0.95)

```
The 95% confidence interval is between 0.147 and 0.179. The true value (0.20) given to us by divine intervention does NOT fall inside this 95% confidence interval.







## Part B - Long Answer Question

### Figure 2
```{r}
# I'm going to start out by making a small corrplot. I recognize this is not necessary for analysis, but it really helps me visualize my data before I begin selection.
corrplotdata <- polypar[c(1,10:14,15,18,21:22, 25:26)]
corrplotdata <- drop_na(corrplotdata)
corrplotdata.cor = cor(corrplotdata)
corrplot(corrplotdata.cor, type = "upper")
# secondary corrplot?
```
Looking at this corrplot, we are trying to generate answers from some relatively weak correlations. This means, making sure our analysis is precise is very important, as small things (like not meeting normality assumptions, etc.) could throw off our recommendations.

## Hypothesis 1 and Mtable Creation

### Figure 1 / Main Model
```{r Hypothesis 1: Social and Economic Attachment,fig.width = 4}
# Base model
model1.1 <- lm(participates ~ more_at_home_village, data = polypar)
model1.2 <-  lm(participates ~ more_at_home_village + owns_home, data = polypar)
model1.3 <-  lm(participates ~ more_at_home_village + owns_home + log(num_calls_to_prior_residence+1), data = polypar)
model1.4 <-  lm(participates ~ more_at_home_village + owns_home + log(num_calls_to_prior_residence+1) + log(kms_to_home + 1), data = polypar)
model1.5 <- lm(participates ~ more_at_home_village + owns_home + log(kms_to_home + 1) + political_efficacy, data = polypar)
model1.6 <- lm(participates ~ more_at_home_village + owns_home + log(kms_to_home + 1) + political_efficacy + reghelp, data = polypar)
model1.7 <- lm(participates ~ more_at_home_village + owns_home + log(kms_to_home + 1) + political_efficacy + encouraged, data = polypar)

# I am removing log(number_of_calls_to_home) due to collinearity. I chose this one due to the lack of normality in the data.

# create mtable, name...
socialandEconMtable <- mtable("Model 1"=model1.1,"Model 2"=model1.2,"Model 3"=model1.3,"Model 4"=model1.4, 'Model 5' = model1.5, 'reghelp experiment'= model1.6, 'encouraged experiment' = model1.7,
                    summary.stats=c("sigma","R-squared","F","p","N"))
socialandEconMtable

# This is actually good -- reghelp has quite strong predictive power, even when included in the model (holding constant for other factors)
# --- arguably should not be included, given that it is an experimental variable. However, it is ok to specify a model with and without it included
# differently, encouraged does not have any predictive power, and is not shown as significant in the model

```




### Figure 3.1, 3.2
```{r Checking regression assumptions for used data, fig.height = 2, fig.width = 4}

# histograms to check for normal distribution and log transform

ggplot(polypar, aes(x=log(num_calls_to_prior_residence + 1))) +
  geom_histogram(bins = 20, fill = 'steelblue') +
  xlab('Number of calls home') +
  ylab('Frequency')

ggplot(polypar, aes(x=log(kms_to_home + 1))) +
  geom_histogram(bins = 20, fill = 'steelblue') +
  xlab('Kilometers to home') +
  ylab('Frequency')

# Let me just note that, while log transforming both of these graphs did make the data slightly better, it is still quite bad overall.
```

Our regression assumptions are largely met, with the exception of normality for the two variables graphed above. For these variables, taking the log transform helped to a degree, but the values are still quite off what we would like to see in a normal bell curve. This does provide some limitations to the model. 

### Figure 4

```{r}
# plots for mtable
par(mfrow = c(2, 2))
plot(model1.4, c(1,2))
plot(model1.6, c(1,2))
```


## Hypothesis 2

### Figure 5, Figure 6
```{r Hypothesis 2: local voter registration processes}
# fit model 2
model2.1 <- lm(participates ~ reghelp + female + educ, data = polypar)
summary(model2.1)

# filter
reghelp_true <- polypar %>%
  filter(reghelp == 1)
reghelp_false <- polypar %>%
  filter(reghelp == 0)

# t test for hypothesis 2
t.test(reghelp_true$participates, reghelp_false$participates, method = 'greater')

```

### Figure 7
```{r}
# plots for model 2
par(mfrow = c(2, 2))
plot(model2.1, c(1,2))
```


## Hypothesis 3

### Figure 8, Figure 9
```{r Hypothesis 3}
# fit model 3
model3.1 <- lm(participates ~ encouraged + female + educ, data = polypar)
summary(model3.1)

encouraged_true <- polypar %>%
  filter(encouraged == 1)
encouraged_false <- polypar %>%
  filter(encouraged == 0)

# t test for hypothesis 3
t.test(encouraged_true$participates, encouraged_false$participates, method = 'greater')
```

### Figure 10
```{r}
# plots for model 3
par(mfrow = c(2, 2))
plot(model3.1, c(1,2))
```

### Figure 11
```{r check for multicollinearity}
library(car)
vif(model1.5)
# These are very satisfyingly low vif values

```


